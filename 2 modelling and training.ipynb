{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2 modelling and training.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-sIy3J6ZilYg"},"source":["# Modelling and Training"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eKygYp_dilYh"},"source":["Now, we are finally able to specify our neural network. To find the right hyperparameters, we use a combination of trying out common and reasonable hyperparameter combinations on the one hand and automated grid search by using the talos package on the other hand. In doing so, we aim to find a good balance between finding the optimal hyperparameters withouth relying on heavy computational power."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KzxZvYKIQBIr"},"source":["As we are dealing with a binary classification and are using the sigmoid function as the activation function in the output layer, it is sufficient to only include one neuron in the output layer. After trying out several other architectures, we decide to use three hidden layers containing 80, 40, and 10 neurons, respectively. We also choose to not include any dropout, as this did not improve our result. However, we included an l2-regularizer to prevent overfitting, which indeed improved our cross-validated performance measure. Furthermore we specified glorot_normal as our method to initialize our weights and biases. Although the He-initializer often is the best choice when using the Relu activation function in the hidden layers, the Glorot initializer turned out to be the better option. We also tried out several optimizers (SGD, Adam, Adadelta, Adagrad, RMSprop, Adamax, Nadam), and decided to use Adam, which naturally incorporates a momentum."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_CVHbe6gilYi","colab":{}},"source":["basemodel = Sequential()\n","basemodel.add(Dense(80, kernel_regularizer=l2(0.01), kernel_initializer=glorot_normal(seed=42), bias_initializer='zeros', activation='relu', input_dim = 125))\n","basemodel.add(Dense(40, kernel_regularizer=l2(0.01), kernel_initializer=glorot_normal(seed=42), bias_initializer='zeros', activation='relu'))\n","basemodel.add(Dense(10, kernel_regularizer=l2(0.01), kernel_initializer=glorot_normal(seed=42), bias_initializer='zeros', activation='relu'))\n","basemodel.add(Dense(1, activation='sigmoid'))\n","\n","basemodel.compile(optimizer=Adam(lr = 0.00005), loss='binary_crossentropy', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MiRBZ7DMilYl"},"source":["After having specified a specific model, we can now train the model. Here, we need to specify two further metaparameters. First, we choose a learning rate of 0.0005, a batch size of 24 and 10 epochs. Further down, we will try to optimize these vaules using the talos package."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":33649,"status":"ok","timestamp":1558121890433,"user":{"displayName":"Gabriel Blumenstock","photoUrl":"","userId":"15396447649265516687"},"user_tz":-120},"id":"k-DFb-S2ilYm","outputId":"774530c0-8d08-45e1-a0d4-ead15809b3cb","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":386}},"source":["story = basemodel.fit(x = X_train, y = y_train, validation_data=[X_test, y_test], epochs = 10, batch_size=24)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 41079 samples, validate on 10270 samples\n","Epoch 1/10\n","41079/41079 [==============================] - 3s 71us/step - loss: 1.5955 - acc: 0.9170 - val_loss: 1.2259 - val_acc: 0.9235\n","Epoch 2/10\n","41079/41079 [==============================] - 3s 63us/step - loss: 0.9907 - acc: 0.9240 - val_loss: 0.7980 - val_acc: 0.9255\n","Epoch 3/10\n","41079/41079 [==============================] - 3s 63us/step - loss: 0.6664 - acc: 0.9252 - val_loss: 0.5601 - val_acc: 0.9271\n","Epoch 4/10\n","41079/41079 [==============================] - 3s 63us/step - loss: 0.4874 - acc: 0.9255 - val_loss: 0.4306 - val_acc: 0.9276\n","Epoch 5/10\n","41079/41079 [==============================] - 3s 63us/step - loss: 0.3902 - acc: 0.9253 - val_loss: 0.3609 - val_acc: 0.9272\n","Epoch 6/10\n","41079/41079 [==============================] - 3s 63us/step - loss: 0.3370 - acc: 0.9249 - val_loss: 0.3221 - val_acc: 0.9248\n","Epoch 7/10\n","41079/41079 [==============================] - 3s 63us/step - loss: 0.3071 - acc: 0.9243 - val_loss: 0.2997 - val_acc: 0.9251\n","Epoch 8/10\n","41079/41079 [==============================] - 3s 63us/step - loss: 0.2894 - acc: 0.9244 - val_loss: 0.2862 - val_acc: 0.9249\n","Epoch 9/10\n","41079/41079 [==============================] - 3s 63us/step - loss: 0.2785 - acc: 0.9246 - val_loss: 0.2776 - val_acc: 0.9248\n","Epoch 10/10\n","41079/41079 [==============================] - 3s 63us/step - loss: 0.2716 - acc: 0.9249 - val_loss: 0.2719 - val_acc: 0.9242\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iRd4BArtCrux"},"source":["To evaluate this model further below, we save the predictions on the test set based on this model:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hvCY58UGiQPB","colab":{}},"source":["predictions = basemodel.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OVFeGl9LR2of"},"source":["Due to the learning rate being a very important hyperparameter and because of its strong connection to the batch size, we now try to find the best values for these hyperparameters by doing grid search with the talos package:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fu8OIXQrSvzs","colab":{}},"source":["import talos as ta"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":229895,"status":"ok","timestamp":1558122086745,"user":{"displayName":"Gabriel Blumenstock","photoUrl":"","userId":"15396447649265516687"},"user_tz":-120},"id":"xs1nYsFbS5o-","outputId":"0aca6091-7c34-494f-dc55-e3fc71517a0e","colab":{"base_uri":"https://localhost:8080/","height":3725}},"source":["X_talos=np.asarray(X)\n","y_talos=np.asarray(y)\n","\n","def minimodel(x_train, y_train, x_val, y_val, params):\n","\n","    model = Sequential()\n","    model.add(Dense(80, kernel_regularizer=l2(0.01), kernel_initializer=glorot_normal(seed=42), bias_initializer='zeros', activation='relu', input_dim = 125))\n","    model.add(Dense(40, kernel_regularizer=l2(0.01), kernel_initializer=glorot_normal(seed=42), bias_initializer='zeros', activation='relu'))\n","    model.add(Dense(10, kernel_regularizer=l2(0.01), kernel_initializer=glorot_normal(seed=42), bias_initializer='zeros', activation='relu'))\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    model.compile(optimizer=Adam(lr = params['learningrate']), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    story = model.fit(x = x_train, y = y_train,\n","          validation_data=[x_val, y_val],\n","          epochs = 10,\n","          batch_size=params['batchsize'])\n","    \n","    return story, model\n","  \n","p = {'learningrate': [0.00001, 0.00003, 0.00005, 0.00007, 0.0001],\n","    'batchsize': [24, 48]}\n","\n","t = ta.Scan(X_talos, y_talos, p, minimodel)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/10 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train on 35944 samples, validate on 15405 samples\n","Epoch 1/10\n","35944/35944 [==============================] - 2s 50us/step - loss: 5.5842 - acc: 0.7270 - val_loss: 2.2795 - val_acc: 0.9182\n","Epoch 2/10\n","35944/35944 [==============================] - 1s 41us/step - loss: 2.0687 - acc: 0.9206 - val_loss: 1.9659 - val_acc: 0.9182\n","Epoch 3/10\n","35944/35944 [==============================] - 1s 41us/step - loss: 1.8380 - acc: 0.9206 - val_loss: 1.8007 - val_acc: 0.9183\n","Epoch 4/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 1.7146 - acc: 0.9207 - val_loss: 1.7110 - val_acc: 0.9183\n","Epoch 5/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 1.6482 - acc: 0.9203 - val_loss: 1.6720 - val_acc: 0.9176\n","Epoch 6/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 1.6066 - acc: 0.9206 - val_loss: 1.6303 - val_acc: 0.9183\n","Epoch 7/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 1.5809 - acc: 0.9205 - val_loss: 1.6061 - val_acc: 0.9181\n","Epoch 8/10\n","35944/35944 [==============================] - 1s 41us/step - loss: 1.5550 - acc: 0.9206 - val_loss: 1.5824 - val_acc: 0.9180\n","Epoch 9/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 1.5349 - acc: 0.9204 - val_loss: 1.5650 - val_acc: 0.9179\n","Epoch 10/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 1.5122 - acc: 0.9206 - val_loss: 1.5410 - val_acc: 0.9183\n"],"name":"stdout"},{"output_type":"stream","text":["\r 10%|█         | 1/10 [00:15<02:20, 15.58s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train on 35944 samples, validate on 15405 samples\n","Epoch 1/10\n","35944/35944 [==============================] - 2s 48us/step - loss: 3.0300 - acc: 0.9080 - val_loss: 2.8417 - val_acc: 0.9183\n","Epoch 2/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 2.7439 - acc: 0.9205 - val_loss: 2.7191 - val_acc: 0.9183\n","Epoch 3/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 2.6263 - acc: 0.9205 - val_loss: 2.6061 - val_acc: 0.9183\n","Epoch 4/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 2.5182 - acc: 0.9206 - val_loss: 2.5021 - val_acc: 0.9183\n","Epoch 5/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 2.4179 - acc: 0.9206 - val_loss: 2.4057 - val_acc: 0.9183\n","Epoch 6/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 2.3248 - acc: 0.9205 - val_loss: 2.3162 - val_acc: 0.9183\n","Epoch 7/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 2.2380 - acc: 0.9205 - val_loss: 2.2347 - val_acc: 0.9183\n","Epoch 8/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 2.1597 - acc: 0.9205 - val_loss: 2.1600 - val_acc: 0.9183\n","Epoch 9/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 2.0906 - acc: 0.9203 - val_loss: 2.0927 - val_acc: 0.9183\n","Epoch 10/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 2.0253 - acc: 0.9206 - val_loss: 2.0315 - val_acc: 0.9183\n"],"name":"stdout"},{"output_type":"stream","text":["\r 20%|██        | 2/10 [00:30<02:03, 15.41s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train on 35944 samples, validate on 15405 samples\n","Epoch 1/10\n","35944/35944 [==============================] - 2s 48us/step - loss: 2.7218 - acc: 0.9205 - val_loss: 2.5622 - val_acc: 0.9184\n","Epoch 2/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 2.3745 - acc: 0.9205 - val_loss: 2.2697 - val_acc: 0.9184\n","Epoch 3/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 2.1216 - acc: 0.9206 - val_loss: 2.0522 - val_acc: 0.9183\n","Epoch 4/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 1.9338 - acc: 0.9206 - val_loss: 1.8915 - val_acc: 0.9184\n","Epoch 5/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 1.7919 - acc: 0.9206 - val_loss: 1.7670 - val_acc: 0.9184\n","Epoch 6/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 1.6881 - acc: 0.9205 - val_loss: 1.6823 - val_acc: 0.9184\n","Epoch 7/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 1.6147 - acc: 0.9205 - val_loss: 1.6204 - val_acc: 0.9184\n","Epoch 8/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 1.5625 - acc: 0.9205 - val_loss: 1.5773 - val_acc: 0.9184\n","Epoch 9/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 1.5250 - acc: 0.9205 - val_loss: 1.5459 - val_acc: 0.9184\n","Epoch 10/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 1.4992 - acc: 0.9205 - val_loss: 1.5246 - val_acc: 0.9184\n"],"name":"stdout"},{"output_type":"stream","text":["\r 30%|███       | 3/10 [00:45<01:47, 15.34s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train on 35944 samples, validate on 15405 samples\n","Epoch 1/10\n","35944/35944 [==============================] - 3s 72us/step - loss: 2.2899 - acc: 0.9206 - val_loss: 1.8700 - val_acc: 0.9184\n","Epoch 2/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 0.8774 - acc: 0.9135 - val_loss: 0.5841 - val_acc: 0.9182\n","Epoch 3/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 0.5048 - acc: 0.9178 - val_loss: 0.4662 - val_acc: 0.9179\n","Epoch 4/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.4393 - acc: 0.9182 - val_loss: 0.4394 - val_acc: 0.9097\n","Epoch 5/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.4150 - acc: 0.9184 - val_loss: 0.4035 - val_acc: 0.9179\n","Epoch 6/10\n","35944/35944 [==============================] - 2s 65us/step - loss: 0.3990 - acc: 0.9188 - val_loss: 0.4466 - val_acc: 0.9184\n","Epoch 7/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 0.3920 - acc: 0.9184 - val_loss: 0.3942 - val_acc: 0.9176\n","Epoch 8/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.3829 - acc: 0.9190 - val_loss: 0.3873 - val_acc: 0.9183\n","Epoch 9/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.3768 - acc: 0.9194 - val_loss: 0.4018 - val_acc: 0.9181\n","Epoch 10/10\n","35944/35944 [==============================] - 2s 62us/step - loss: 0.3713 - acc: 0.9193 - val_loss: 0.3699 - val_acc: 0.9176\n"],"name":"stdout"},{"output_type":"stream","text":["\r 40%|████      | 4/10 [01:09<01:46, 17.83s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train on 35944 samples, validate on 15405 samples\n","Epoch 1/10\n","35944/35944 [==============================] - 3s 72us/step - loss: 2.4967 - acc: 0.9206 - val_loss: 2.1756 - val_acc: 0.9184\n","Epoch 2/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 1.8908 - acc: 0.9200 - val_loss: 1.0903 - val_acc: 0.8810\n","Epoch 3/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.6803 - acc: 0.9168 - val_loss: 0.5681 - val_acc: 0.9179\n","Epoch 4/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.5175 - acc: 0.9197 - val_loss: 0.4834 - val_acc: 0.9174\n","Epoch 5/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 0.4568 - acc: 0.9193 - val_loss: 0.4418 - val_acc: 0.9184\n","Epoch 6/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.4264 - acc: 0.9198 - val_loss: 0.4227 - val_acc: 0.9183\n","Epoch 7/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.4128 - acc: 0.9191 - val_loss: 0.4118 - val_acc: 0.9169\n","Epoch 8/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.4028 - acc: 0.9198 - val_loss: 0.3992 - val_acc: 0.9179\n","Epoch 9/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 0.3935 - acc: 0.9199 - val_loss: 0.3979 - val_acc: 0.9183\n","Epoch 10/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.3879 - acc: 0.9203 - val_loss: 0.3922 - val_acc: 0.9171\n"],"name":"stdout"},{"output_type":"stream","text":["\r 50%|█████     | 5/10 [01:33<01:38, 19.65s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train on 35944 samples, validate on 15405 samples\n","Epoch 1/10\n","35944/35944 [==============================] - 2s 48us/step - loss: 2.5583 - acc: 0.9206 - val_loss: 2.2770 - val_acc: 0.9184\n","Epoch 2/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 2.0215 - acc: 0.9206 - val_loss: 1.8703 - val_acc: 0.9184\n","Epoch 3/10\n","35944/35944 [==============================] - 1s 38us/step - loss: 1.7041 - acc: 0.9206 - val_loss: 1.6283 - val_acc: 0.9184\n","Epoch 4/10\n","35944/35944 [==============================] - 1s 38us/step - loss: 0.9912 - acc: 0.9206 - val_loss: 0.4787 - val_acc: 0.9184\n","Epoch 5/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 0.4393 - acc: 0.9206 - val_loss: 0.4135 - val_acc: 0.9184\n","Epoch 6/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 0.3869 - acc: 0.9206 - val_loss: 0.3735 - val_acc: 0.9184\n","Epoch 7/10\n","35944/35944 [==============================] - 2s 44us/step - loss: 0.3581 - acc: 0.9206 - val_loss: 0.3558 - val_acc: 0.9184\n","Epoch 8/10\n","35944/35944 [==============================] - 2s 45us/step - loss: 0.3412 - acc: 0.9206 - val_loss: 0.3429 - val_acc: 0.9184\n","Epoch 9/10\n","35944/35944 [==============================] - 2s 45us/step - loss: 0.3307 - acc: 0.9206 - val_loss: 0.3303 - val_acc: 0.9184\n","Epoch 10/10\n","35944/35944 [==============================] - 2s 44us/step - loss: 0.3225 - acc: 0.9206 - val_loss: 0.3352 - val_acc: 0.9184\n"],"name":"stdout"},{"output_type":"stream","text":["\r 60%|██████    | 6/10 [01:48<01:13, 18.46s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train on 35944 samples, validate on 15405 samples\n","Epoch 1/10\n","35944/35944 [==============================] - 3s 81us/step - loss: 16.1649 - acc: 0.0794 - val_loss: 15.9732 - val_acc: 0.0816\n","Epoch 2/10\n","35944/35944 [==============================] - 2s 65us/step - loss: 15.8710 - acc: 0.0794 - val_loss: 15.7081 - val_acc: 0.0816\n","Epoch 3/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 15.6297 - acc: 0.0794 - val_loss: 15.4889 - val_acc: 0.0816\n","Epoch 4/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 15.4299 - acc: 0.0794 - val_loss: 15.3076 - val_acc: 0.0816\n","Epoch 5/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 12.1201 - acc: 0.2477 - val_loss: 1.0365 - val_acc: 0.8953\n","Epoch 6/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.8642 - acc: 0.9076 - val_loss: 0.7846 - val_acc: 0.9157\n","Epoch 7/10\n","35944/35944 [==============================] - 2s 65us/step - loss: 0.7351 - acc: 0.9182 - val_loss: 0.6983 - val_acc: 0.9174\n","Epoch 8/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 0.6600 - acc: 0.9188 - val_loss: 0.6340 - val_acc: 0.9175\n","Epoch 9/10\n","35944/35944 [==============================] - 2s 65us/step - loss: 0.6035 - acc: 0.9197 - val_loss: 0.5857 - val_acc: 0.9178\n","Epoch 10/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 0.5609 - acc: 0.9199 - val_loss: 0.5482 - val_acc: 0.9179\n"],"name":"stdout"},{"output_type":"stream","text":["\r 70%|███████   | 7/10 [02:13<01:00, 20.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train on 35944 samples, validate on 15405 samples\n","Epoch 1/10\n","35944/35944 [==============================] - 2s 48us/step - loss: 15.3519 - acc: 0.0973 - val_loss: 1.7924 - val_acc: 0.9184\n","Epoch 2/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 0.8249 - acc: 0.9169 - val_loss: 0.5924 - val_acc: 0.9170\n","Epoch 3/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 0.5283 - acc: 0.9192 - val_loss: 0.4976 - val_acc: 0.9181\n","Epoch 4/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 0.4617 - acc: 0.9194 - val_loss: 0.4557 - val_acc: 0.9178\n","Epoch 5/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 0.4381 - acc: 0.9189 - val_loss: 0.4285 - val_acc: 0.9179\n","Epoch 6/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 0.4185 - acc: 0.9199 - val_loss: 0.4157 - val_acc: 0.9183\n","Epoch 7/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 0.4052 - acc: 0.9197 - val_loss: 0.4086 - val_acc: 0.9186\n","Epoch 8/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 0.3941 - acc: 0.9199 - val_loss: 0.3935 - val_acc: 0.9187\n","Epoch 9/10\n","35944/35944 [==============================] - 1s 39us/step - loss: 0.3905 - acc: 0.9201 - val_loss: 0.3900 - val_acc: 0.9161\n","Epoch 10/10\n","35944/35944 [==============================] - 1s 40us/step - loss: 0.3774 - acc: 0.9202 - val_loss: 0.3828 - val_acc: 0.9164\n"],"name":"stdout"},{"output_type":"stream","text":["\r 80%|████████  | 8/10 [02:28<00:37, 18.67s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train on 35944 samples, validate on 15405 samples\n","Epoch 1/10\n","35944/35944 [==============================] - 3s 72us/step - loss: 2.1467 - acc: 0.9184 - val_loss: 1.7278 - val_acc: 0.9183\n","Epoch 2/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 1.5764 - acc: 0.9206 - val_loss: 1.5486 - val_acc: 0.9183\n","Epoch 3/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 1.4824 - acc: 0.9206 - val_loss: 1.4946 - val_acc: 0.9183\n","Epoch 4/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 1.4374 - acc: 0.9206 - val_loss: 1.4525 - val_acc: 0.9184\n","Epoch 5/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 1.3989 - acc: 0.9206 - val_loss: 1.4231 - val_acc: 0.9184\n","Epoch 6/10\n","35944/35944 [==============================] - 2s 61us/step - loss: 1.3823 - acc: 0.9206 - val_loss: 1.4112 - val_acc: 0.9184\n","Epoch 7/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 1.3698 - acc: 0.9206 - val_loss: 1.3977 - val_acc: 0.9184\n","Epoch 8/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 1.3557 - acc: 0.9206 - val_loss: 1.3815 - val_acc: 0.9184\n","Epoch 9/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.4998 - acc: 0.9152 - val_loss: 0.3571 - val_acc: 0.9168\n","Epoch 10/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 0.3647 - acc: 0.9173 - val_loss: 0.4319 - val_acc: 0.9183\n"],"name":"stdout"},{"output_type":"stream","text":["\r 90%|█████████ | 9/10 [02:51<00:20, 20.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train on 35944 samples, validate on 15405 samples\n","Epoch 1/10\n","35944/35944 [==============================] - 3s 72us/step - loss: 11.4659 - acc: 0.3170 - val_loss: 0.7297 - val_acc: 0.9164\n","Epoch 2/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.5843 - acc: 0.9182 - val_loss: 0.5202 - val_acc: 0.9181\n","Epoch 3/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.4575 - acc: 0.9193 - val_loss: 0.4336 - val_acc: 0.9168\n","Epoch 4/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 0.4179 - acc: 0.9200 - val_loss: 0.4094 - val_acc: 0.9181\n","Epoch 5/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.3997 - acc: 0.9197 - val_loss: 0.3942 - val_acc: 0.9179\n","Epoch 6/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.3820 - acc: 0.9200 - val_loss: 0.4101 - val_acc: 0.9181\n","Epoch 7/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 0.3720 - acc: 0.9201 - val_loss: 0.3718 - val_acc: 0.9184\n","Epoch 8/10\n","35944/35944 [==============================] - 2s 63us/step - loss: 0.3608 - acc: 0.9200 - val_loss: 0.3652 - val_acc: 0.9177\n","Epoch 9/10\n","35944/35944 [==============================] - 2s 64us/step - loss: 0.3611 - acc: 0.9192 - val_loss: 0.3518 - val_acc: 0.9174\n","Epoch 10/10\n","35944/35944 [==============================] - 2s 67us/step - loss: 0.3506 - acc: 0.9198 - val_loss: 0.3507 - val_acc: 0.9184\n"],"name":"stdout"},{"output_type":"stream","text":["\r100%|██████████| 10/10 [03:15<00:00, 21.30s/it]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"I2MMHO_iTJLv"},"source":["Let´s have a look at the results of our grid search:"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":229867,"status":"ok","timestamp":1558122086752,"user":{"displayName":"Gabriel Blumenstock","photoUrl":"","userId":"15396447649265516687"},"user_tz":-120},"id":"QhHrgZTrS5n6","outputId":"b42c9a0d-d9fd-4fc6-9681-464c05884170","colab":{"base_uri":"https://localhost:8080/","height":245}},"source":["print(t.data)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   round_epochs  val_loss   val_acc  ...       acc  learningrate  batchsize\n","0            10  1.540983  0.918338  ...  0.920654       0.00007         48\n","1            10  2.031460  0.918338  ...  0.920627       0.00001         48\n","2            10  1.524614  0.918403  ...  0.920571       0.00003         48\n","3            10  0.369941  0.918403  ...  0.920571       0.00005         24\n","4            10  0.392205  0.918403  ...  0.920571       0.00003         24\n","5            10  0.330297  0.918403  ...  0.920571       0.00005         48\n","6            10  0.548199  0.917884  ...  0.919931       0.00001         24\n","7            10  0.382778  0.918663  ...  0.920154       0.00010         48\n","8            10  0.357133  0.918403  ...  0.920627       0.00010         24\n","9            10  0.350741  0.918403  ...  0.920126       0.00007         24\n","\n","[10 rows x 7 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yztGn_JFTqbf"},"source":["When looking at the different accuracies on the validation set, our initial guess of an learning rate of 0.00005 and a batch size of 24 seems to be a reasonable choice. Indeed, when trying out different combinations, the crossvalidated AUC seems to be maximized when using these vaules and we therefore stick to them."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r4L-RlrdvlYe"},"source":["One more issue which we need to consider is the fact, that the dataset is quite imbalanced: There are far more cases where users do not purchase than cases in which they do purchase an item. This might be a problem as our model might become biased towards the majority class. One possible way to face this problem is to oversample the minority class. We do this by implementing the popular SMOTE-algorithm. However, this method does not seem to improve our crossvalidated result. We therefore stick to train our model with the original, imbalanced dataset."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":269686,"status":"ok","timestamp":1558122126599,"user":{"displayName":"Gabriel Blumenstock","photoUrl":"","userId":"15396447649265516687"},"user_tz":-120},"id":"NBLMojM0vjxh","outputId":"67e1f144-76e3-40ac-ac2d-729faf8beac7","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["print(minidata.groupby('Kauf').size())\n","X_train_smote, y_train_smote = SMOTE(random_state=12, ratio = 1.0).fit_sample(X_train, y_train)\n","X_train_smote = pd.DataFrame(X_train_smote)\n","y_train_smote = pd.DataFrame(y_train_smote)\n","smotemodel = Sequential()\n","smotemodel.add(Dense(80, kernel_regularizer=l2(0.01), kernel_initializer=glorot_normal(seed=42), bias_initializer='zeros', activation='relu', input_dim = 125))\n","smotemodel.add(Dense(40, kernel_regularizer=l2(0.01), kernel_initializer=glorot_normal(seed=42), bias_initializer='zeros', activation='relu'))\n","smotemodel.add(Dense(10, kernel_regularizer=l2(0.01), kernel_initializer=glorot_normal(seed=42), bias_initializer='zeros', activation='relu'))\n","smotemodel.add(Dense(1, activation='sigmoid'))\n","smotemodel.compile(optimizer=Adam(lr = 0.00005), loss='binary_crossentropy', metrics=['accuracy'])\n","story2 = smotemodel.fit(x = X_train_smote, y = y_train_smote, validation_data=[X_test, y_test], epochs = 10, batch_size=24, verbose=0)\n","predictions_smote = smotemodel.predict(X_test)\n","print('AUC: ', metrics.roc_auc_score(y_test, predictions_smote))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Kauf\n","0    47237\n","1     4112\n","dtype: int64\n","AUC:  0.7263959240711243\n"],"name":"stdout"}]}]}